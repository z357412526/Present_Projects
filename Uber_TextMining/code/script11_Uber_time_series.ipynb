{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import twitter\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import string\n",
    "from time import sleep\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONSUMER_KEY = 'mn4YRu8l9vnoGtIwDn8wDBhJb'\n",
    "CONSUMER_SECRET = 'vB2CgIWUy6f27Gw806d6HQ4WhQMxKgz06kLybZPcwH6sL7PnJ9'\n",
    "OAUTH_TOKEN = '4804645274-F9D6CIuGlOY4uIthQh1IijuRLLKGEt2aJ84Q8Al'\n",
    "OAUTH_TOKEN_SECRET = 'RPqhg5a14PzhCe1NdrpI3S4XOX2kGVtT0fm7LJSc514jz'\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "api = twitter.Twitter(auth=auth)\n",
    "\n",
    "def retrieve_tag(key_word):\n",
    "\n",
    "    print(\"Beginning retrieval of \" + key_word)\n",
    "    try:\n",
    "        timeline = api.search.tweets(q=key_word, lang='en', count=100)\n",
    "    except:\n",
    "        print(\"Reached rate limit; sleeping 15 minutes\")\n",
    "        sleep(900)\n",
    "        timeline = api.search.tweets(q=key_word, lang='en', count=100)\n",
    "\n",
    "    ntweets = len(timeline['statuses'])\n",
    "    if ntweets == 0:\n",
    "        return timeline\n",
    "    #### change 30000 to the total number of tweets you want\n",
    "    while ntweets > 0 and len(timeline['statuses']) < 30000:\n",
    "        min_id = min([tweet[\"id\"] for tweet in timeline['statuses']])\n",
    "        try:\n",
    "            next_timeline = api.search.tweets(q=key_word, lang='en', count=100, max_id=min_id - 1)\n",
    "        except:\n",
    "            print(\"Reached rate limit; sleeping 15 minutes\")\n",
    "            sleep(900)\n",
    "            print(\"Restarting\")\n",
    "            next_timeline = api.search.tweets(q=key_word, lang='en', count=100, max_id=min_id - 1)\n",
    "\n",
    "        ntweets = len(next_timeline['statuses'])\n",
    "        timeline['statuses'] += next_timeline['statuses']\n",
    "    return timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning retrieval of #uber\n",
      "Reached rate limit; sleeping 15 minutes\n",
      "Restarting\n"
     ]
    }
   ],
   "source": [
    "uber = retrieve_tag('#uber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30023"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uber['statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"hashtagUber.json\", \"w\") as outfile:\n",
    "    json.dump(uber, outfile, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"hashtagUber.json\") as infile:\n",
    "    tweet_uber = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30023"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_uber['statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['statuses', 'search_metadata'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_uber.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hashtags': [{'indices': [19, 24], 'text': 'uber'},\n",
       "  {'indices': [45, 51], 'text': 'toxic'},\n",
       "  {'indices': [67, 74], 'text': 'ubered'}],\n",
       " 'symbols': [],\n",
       " 'urls': [{'display_url': 'twitter.com/taximassive/stâ€¦',\n",
       "   'expanded_url': 'https://twitter.com/taximassive/status/695011911725875200',\n",
       "   'indices': [75, 98],\n",
       "   'url': 'https://t.co/oB4Q7zB7dQ'}],\n",
       " 'user_mentions': [{'id': 3001518083,\n",
       "   'id_str': '3001518083',\n",
       "   'indices': [3, 17],\n",
       "   'name': 'PETE B',\n",
       "   'screen_name': 'peterbyrne822'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_uber['statuses'][1]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [a['text'] for a in tweet_uber['statuses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = [a['created_at'] for a in tweet_uber['statuses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = ','.join(texts) # convert list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = preprocess(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-64d18f12d304>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokens.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"tokens.json\", \"w\") as outfile:\n",
    "    json.dump(tokens, outfile, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"tokens.json\") as infile:\n",
    "    tokens = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_all = Counter()\n",
    "    \n",
    "# Update the counter\n",
    "count_all.update(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 20975), ('#Uber', 20335), ('.', 19541), (':', 16499), ('to', 8974)]\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via','â€¦']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_punct(x):\n",
    "    return(all([char in stop for char in x]))\n",
    "\n",
    "def my_tokenize(text):\n",
    "    return([w for w in tokens if not all_punct(w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#Uber', 20335),\n",
       " ('#uber', 7959),\n",
       " ('#taxis', 2670),\n",
       " ('#SuperBowl50', 2398),\n",
       " ('#Free', 2394),\n",
       " ('#FREERIDE', 2388),\n",
       " ('#Code', 2387),\n",
       " ('#RIDE', 2386),\n",
       " ('#Promo', 2386),\n",
       " ('#GOPATS', 2384),\n",
       " ('#SALE', 2384),\n",
       " ('#PATRIOTS', 2384),\n",
       " ('#WINTER', 2384),\n",
       " ('#SB50', 1824),\n",
       " ('#UBER', 1362),\n",
       " ('#Paris', 737),\n",
       " ('#promocode', 673),\n",
       " ('#UberTaxiWars', 666),\n",
       " ('#tfl', 587),\n",
       " ('#yegcc', 550),\n",
       " ('#startup', 543),\n",
       " ('#Lyft', 496),\n",
       " ('#France', 491),\n",
       " ('#lyft', 477),\n",
       " ('#NYC', 444),\n",
       " (\"#Uber's\", 443),\n",
       " ('#taxi', 432),\n",
       " ('#yeg', 398),\n",
       " ('#branding', 368),\n",
       " ('#tech', 367),\n",
       " ('#FREE', 306),\n",
       " ('#logo', 294),\n",
       " ('#free', 266),\n",
       " ('#weed', 248),\n",
       " ('#rideshare', 240),\n",
       " ('#Edmonton', 235),\n",
       " ('#ubered', 233),\n",
       " ('#design', 218),\n",
       " ('#London', 212),\n",
       " ('#Taxi', 196),\n",
       " ('#clashes', 189),\n",
       " ('#Clashes', 179),\n",
       " ('#uberlogo', 172),\n",
       " ('#uberstrike', 171),\n",
       " ('#greve', 169),\n",
       " ('#NBA', 157),\n",
       " ('#Tuscaloosa', 156),\n",
       " ('#minicab', 150),\n",
       " ('#travel', 149),\n",
       " ('#sharingeconomy', 140)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = my_tokenize(texts)\n",
    "tokens = nltk.Text(tokens)\n",
    "hashtagtokens = [a for a in tokens if a[0] == '#']\n",
    "fdist = FreqDist(hashtagtokens)\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
